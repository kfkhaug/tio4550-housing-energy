{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918f816f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tio4550_housing_energy.utils as u\n",
    "\n",
    "# This will now work!\n",
    "u.a_test_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafbc7f",
   "metadata": {},
   "source": [
    "# Test of OpenAI's Responses API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef724628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file was not found at ../data/2_interim/relevant_ads_sampled_for_chatgpt.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {}  \u001b[38;5;66;03m# Return empty dict on API failure\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# --- 5. PROCESS THE DATAFRAME IN PARALLEL ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m descriptions = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m].tolist()\n\u001b[32m    100\u001b[39m api_results = [{} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m descriptions]  \u001b[38;5;66;03m# Initialize with empty dicts\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(\n\u001b[32m    103\u001b[39m     max_workers=\u001b[32m5\u001b[39m\n\u001b[32m    104\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m executor:  \u001b[38;5;66;03m# Increased workers slightly for speed\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --- 1. SETUP & AUTHENTICATION ---\n",
    "# Load the environment variable for the API key from a .env file.\n",
    "load_dotenv()\n",
    "\n",
    "# Instantiate the OpenAI client.\n",
    "# It will automatically use the OPENAI_API_KEY from your .env file.\n",
    "try:\n",
    "    client = OpenAI()\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        raise ValueError(\"OPENAI_API_KEY environment variable not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    print(\n",
    "        \"Please ensure your OPENAI_API_KEY is set correctly as an environment variable.\"\n",
    "    )\n",
    "    exit()\n",
    "\n",
    "# --- 2. PROMPT ---\n",
    "system_prompt = \"\"\"\n",
    "Du er en svært nøyaktig og detaljorientert ekspert på analyse av norske boligannonser. Oppgaven din er å identifisere og trekke ut relevant informasjon om variabler fra boligannonser basert på spesifikke instrukser og returnere det som et JSON objekt. \n",
    "\n",
    "**Generelle regler:**\n",
    "1. Hvis en egenskap ikke er nevnt, settes dens _bool-verdi til 0 og dens _år-verdi til 0. \n",
    "2. Nevnes en egenskap uten tilhørende årstall, settes dens _bool-verdi til 1 og _år til 0. \n",
    "3. Nevnes både egenskap og årstall, settes _bool til 1 og _år til oppgitt årstall. \n",
    "4. Ved flere årstall for samme egenskap, bruk det nyeste. \n",
    "5. Ditt endelige resultat MÅ være et enkelt, gyldig JSON-objekt med KUN nøklene spesifisert nedenfor. \n",
    "\n",
    "**Spesifikke regler:**\n",
    "1. ot_bool (Totalrenovering): Sett til 1 KUN \"totalrenovering” er beskrevet i detalj. Ignorer generelle eller vage fraser som \"omfattende renovert\" dersom de kun beskriver kosmetiske endringer. Da skal ot_bool være 0. \n",
    "2. t_bool (Tak): Sett til 1 kun ved konkret bytte eller nytt tak (eksempel: \"nytt tak\"). Ikke vedlikehold, inspeksjon eller rens. \n",
    "3. k_bool (Kledning): Sett til 1 kun for bytte av ytre veggpanel (eksempel: \"ny kledning\"). Maling teller ikke. \n",
    "4. e_bool (Etterisolering): Sett til 1 ved forbedring av isolasjon (eksempel: \"etterisolert\"). Bytte av vinduer, kledning eller gulv telles ikke. \n",
    "5. v_bool (Vinduer): Sett til 1 hvis vinduer er byttet (eksempel: \"nye vinduer\"). Bytte av kun glass eller maling av karm teller ikke. \n",
    "6. ob_bool (Bad), ok_bool (Kjøkken), og_bool (Gulv): Sett til 1 hvis det er nevnt nytt, oppgradert eller byttet. \n",
    "7. vp_bool (Varmepumpe): Sett til 1 hvis installasjon av varmepumpe er nevnt. \n",
    "\n",
    "**JSON Schema:**\n",
    "- `vp_bool`, `vp_år`: (Indikerer om varmepumpe er installert/årstall) \n",
    "- `e_bool`, `e_år`: (Etterisolering gjennomført/årstall) \n",
    "- `v_bool`, `v_år`: (Nye eller oppgraderte vinduer/årstall) \n",
    "- `k_bool`, `k_år`: (Ny kledning på yttervegger/årstall) \n",
    "- `t_bool`, `t_år`: (Tak byttet eller oppgradert/årstall) \n",
    "- `ot_bool`, `ot_år`: (Totalrenovering er gjennomført/årstall) \n",
    "- `ob_bool`, `ob_år`: (Oppgradering av bad/årstall) \n",
    "- `ok_bool`, `ok_år`: (Oppgradering av kjøkken/årstall) \n",
    "- `og_bool`, `og_år`: (Oppgradering av gulv/årstall) \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# --- 3. LOAD DATASET ---\n",
    "input_data_path = \"../data/2_interim/relevant_ads_sampled.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(input_data_path, delimiter=\";\")\n",
    "    print(f\"Successfully loaded {len(df)} ads from {input_data_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {input_data_path}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 4. DEFINE API CALL FUNCTION ---\n",
    "def analyze_ad_description(description):\n",
    "    \"\"\"Calls the OpenAI API. Includes robust error handling.\"\"\"\n",
    "    if not isinstance(description, str) or not description.strip():\n",
    "        return {}  # Return an empty dict for invalid input\n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5-nano\",\n",
    "            reasoning={\"effort\": \"medium\"},\n",
    "            # temperature=0.1, TODO Find a parameter that controls temperature. Apparently, \"Temperature\" dosent work right now.\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": description},\n",
    "            ],\n",
    "            text={\n",
    "                \"format\": {\"type\": \"json_object\"},\n",
    "            },\n",
    "        )\n",
    "        json_output = response.output_text\n",
    "        return json.loads(json_output)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Warning: Failed to parse JSON from API response for one ad.\")\n",
    "        return {}  # Return empty dict if JSON is malformed\n",
    "    except Exception as e:\n",
    "        print(f\"An API error occurred: {e}\")\n",
    "        return {}  # Return empty dict on API failure\n",
    "\n",
    "\n",
    "# --- 5. PROCESS THE DATAFRAME IN PARALLEL ---\n",
    "descriptions = df[\"description\"].tolist()\n",
    "api_results = [{} for _ in descriptions]  # Initialize with empty dicts\n",
    "\n",
    "with ThreadPoolExecutor(\n",
    "    max_workers=5\n",
    ") as executor:  # Increased workers slightly for speed\n",
    "    future_to_idx = {\n",
    "        executor.submit(analyze_ad_description, desc): idx\n",
    "        for idx, desc in enumerate(descriptions)\n",
    "    }\n",
    "    for future in tqdm(\n",
    "        as_completed(future_to_idx), total=len(descriptions), desc=\"Analyzing Ads\"\n",
    "    ):\n",
    "        idx = future_to_idx[future]\n",
    "        try:\n",
    "            api_results[idx] = future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing result for index {idx}: {e}\")\n",
    "            api_results[\n",
    "                idx\n",
    "            ] = {}  # Ensure it's an empty dict on any future-related error\n",
    "\n",
    "# --- 6. ROBUSTLY PARSE RESULTS AND SAVE ---\n",
    "# Define all the columns you expect to get from the API\n",
    "expected_columns = [\n",
    "    \"vp_bool\",\n",
    "    \"vp_år\",\n",
    "    \"el_bool\",\n",
    "    \"el_år\",\n",
    "    \"e_bool\",\n",
    "    \"e_år\",\n",
    "    \"v_bool\",\n",
    "    \"v_år\",\n",
    "    \"k_bool\",\n",
    "    \"k_år\",\n",
    "    \"t_bool\",\n",
    "    \"t_år\",\n",
    "    \"s_bool\",\n",
    "    \"s_år\",\n",
    "    \"ot_bool\",\n",
    "    \"ot_år\",\n",
    "    \"ob_bool\",\n",
    "    \"ob_år\",\n",
    "    \"ok_bool\",\n",
    "    \"ok_år\",\n",
    "    \"og_bool\",\n",
    "    \"og_år\",\n",
    "]\n",
    "\n",
    "# Create a clean list of dictionaries, ensuring every dictionary has all expected keys\n",
    "clean_results = []\n",
    "for result in api_results:\n",
    "    clean_dict = {}\n",
    "    for col in expected_columns:\n",
    "        # Use .get() with a default value of 0 to handle missing keys gracefully\n",
    "        clean_dict[col] = result.get(col, 0)\n",
    "    clean_results.append(clean_dict)\n",
    "\n",
    "# Create the results DataFrame from the clean list\n",
    "results_df = pd.DataFrame(clean_results)\n",
    "\n",
    "# Combine the original DataFrame with the new results DataFrame\n",
    "final_df = pd.concat([df, results_df], axis=1)\n",
    "\n",
    "# --- THIS IS THE NEW STEP ---\n",
    "# Drop the original text columns as they are no longer needed for the final dataset\n",
    "final_df = final_df.drop(columns=[\"description\", \"title\"])\n",
    "\n",
    "# Define the output path\n",
    "output_path = \"../data/3_processed/analyzed_ads_data_clean.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Save the final DataFrame\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Processing complete! Clean data saved to {output_path}\")\n",
    "print(\"\\nHere's a preview of the clean final data (without description and title):\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tio4550-housing-energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
